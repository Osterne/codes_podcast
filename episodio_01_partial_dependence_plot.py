# -*- coding: utf-8 -*-
"""Episodio 01: Partial Dependence Plot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezxUO7IgE7rASF4FSxqMJoslkRw2jsFs

# The Data Science Podcast (por Vinícius Osterne)
## Episódio 01 - Partial Dependence Plot

## Sobre o problema

Quando trabalhamos com modelos de árvore, como o GBM, é comum alguém olhar para um atributo na base de dados e pensar: 'Ah, essa variável parece importante porque, em média, valores altos aumentam a resposta…' Mas isso é um erro clássico.

A verdade é que o modelo pode estar interpretando esse atributo de uma forma completamente diferente do que vemos olhando só para os dados crus. E é aí que entra o Partial Dependence Plot, ou PDP.

## Exemplo fictício:

*   Número de transações nas últimas 24h – Detecta padrões anormais de atividade.
*   Número de dispositivos diferentes usados para login – Indica possível comprometimento de conta.
*   Tentativas de senha incorreta – Comportamento típico de ataques ou fraude.

## Análise
"""

!pip install h2o

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.inspection import PartialDependenceDisplay
import matplotlib.pyplot as plt
import h2o
from h2o.estimators import H2OGradientBoostingEstimator

# ===============================
# 1. Criar base artificial
# ===============================
import numpy as np
import pandas as pd

np.random.seed(42)
n_samples = 2000

# Variáveis comportamentais
transacoes_24h = np.random.poisson(lam=2, size=n_samples)               # discreta
dispositivos = np.random.randint(1, 5, n_samples)                       # discreta
tentativas_senha = np.random.poisson(lam=1, size=n_samples)              # discreta

# Variáveis contínuas
tempo_sessao = np.random.exponential(scale=300, size=n_samples)          # tempo de sessão em segundos
valor_medio_transacao = np.random.normal(loc=500, scale=200, size=n_samples)  # valor médio em R$

# Ajuste para manter valores positivos
valor_medio_transacao = np.clip(valor_medio_transacao, 50, None)

# Lógica da fraude:
# - Fraudes ocorrem com muitas tentativas de senha e dispositivos diferentes
# - Pouquíssimas transações (fraude discreta)
# - Sessão muito longa ou valor médio muito alto também aumenta risco
fraude = (
    ((tentativas_senha > 2) & (dispositivos > 2)) |
    ((transacoes_24h < 1) & (tentativas_senha > 0)) |
    (tempo_sessao > 800) |
    (valor_medio_transacao > 1000)
).astype(int)

df = pd.DataFrame({
    'transacoes_24h': transacoes_24h,
    'dispositivos': dispositivos,
    'tentativas_senha': tentativas_senha,
    'tempo_sessao': tempo_sessao,
    'valor_medio_transacao': valor_medio_transacao,
    'fraude': fraude
})

df.head(5)

df.head(5)

# ===============================
# 2. Análise descritiva
# ===============================
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def plot_bivariada_misto(df, variavel, bins=10):
    # Se a variável for contínua, criar faixas
    if pd.api.types.is_numeric_dtype(df[variavel]) and df[variavel].nunique() > 20:
        df_plot = df.copy()
        df_plot[variavel + '_bin'] = pd.cut(df_plot[variavel], bins=bins)
        total = df_plot[variavel + '_bin'].value_counts().sort_index()
        pct_fraude = (
            df_plot[df_plot['fraude'] == 1][variavel + '_bin'].value_counts().sort_index()
            / total
        ) * 100
        x_labels = [f"{interval.left:.0f}-{interval.right:.0f}" for interval in total.index]
    else:
        total = df[variavel].value_counts().sort_index()
        pct_fraude = (
            df[df['fraude'] == 1][variavel].value_counts().sort_index() / total
        ) * 100
        x_labels = total.index.astype(str)

    # Criando gráfico misto
    fig, ax1 = plt.subplots(figsize=(10, 4))

    # Barras - Total
    ax1.bar(range(len(total)), total.values, color='skyblue', label='Total')
    ax1.set_xlabel(variavel)
    ax1.set_ylabel('Total', color='blue')
    ax1.tick_params(axis='y', labelcolor='blue')
    ax1.set_xticks(range(len(total)))
    ax1.set_xticklabels(x_labels, rotation=45)

    # Linha - % Fraude
    ax2 = ax1.twinx()
    ax2.plot(range(len(pct_fraude)), pct_fraude.values, color='red', marker='o', label='% Fraude')
    ax2.set_ylabel('% Fraude', color='red')
    ax2.tick_params(axis='y', labelcolor='red')

    plt.title(f'Relação de Total x % Fraude para {variavel}')
    fig.tight_layout()
    plt.show()

# Gerar gráfico misto para todas as variáveis
plot_bivariada_misto(df, 'transacoes_24h')
plot_bivariada_misto(df, 'dispositivos')
plot_bivariada_misto(df, 'tentativas_senha')
plot_bivariada_misto(df, 'tempo_sessao')
plot_bivariada_misto(df, 'valor_medio_transacao')

# ===============================
# 3. Treinar modelo GBM
# ===============================
# Iniciar H2O
h2o.init()

# Converter o dataframe para H2OFrame
hf = h2o.H2OFrame(df)

# Definir features e target (agora incluindo as contínuas)
features = [
    'transacoes_24h',
    'dispositivos',
    'tentativas_senha',
    'tempo_sessao',
    'valor_medio_transacao'
]
target = 'fraude'

# Converter target para categórico (fator)
hf[target] = hf[target].asfactor()

# Treinar modelo GBM
gbm_model = H2OGradientBoostingEstimator(
    ntrees=100,
    max_depth=3,
    learn_rate=0.1,
    seed=42
)
gbm_model.train(x=features, y=target, training_frame=hf)

# ===============================
# 4. Importância das variáveis
# ===============================
# Importância das variáveis
importancia = gbm_model.varimp(use_pandas=True)

# Mostrar
print(importancia)

# ===============================
# 5. PDP para cada variável
# ===============================
import h2o
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Lista de variáveis do modelo (removendo target)
features = gbm_model._model_json['output']['names']
features.remove('fraude')

# Loop para todas as variáveis
for feature in features:
    try:
        # PDP para a variável
        pdp_result = gbm_model.partial_plot(
            data=hf,
            cols=[feature],
            plot=False
        )

        # Converter PDP para pandas
        pdp_df = pd.DataFrame(pdp_result[0].as_data_frame())

        # Calcular taxa real de fraude usando os mesmos bins
        df_real = df.copy()
        if feature not in df_real.columns:
            continue  # pula se a variável não existe no dataset original

        df_real[f'{feature}_bin'] = pd.cut(
            df_real[feature],
            bins=np.unique(pdp_df[feature]),
            include_lowest=True
        )
        fraud_rate_real = (
            df_real.groupby(f'{feature}_bin')['fraude']
            .mean()
            .reset_index()
        )
        fraud_rate_real['midpoint'] = fraud_rate_real[f'{feature}_bin'].apply(lambda x: x.mid)

        # Plot
        plt.figure(figsize=(10, 5))
        plt.plot(pdp_df[feature], pdp_df['mean_response'],
                 marker='o', label='PDP - Modelo (Prob. prevista)')
        plt.fill_between(
            pdp_df[feature],
            pdp_df['mean_response'] - pdp_df['std_error_mean_response'],
            pdp_df['mean_response'] + pdp_df['std_error_mean_response'],
            color='blue',
            alpha=0.2
        )

        plt.plot(fraud_rate_real['midpoint'], fraud_rate_real['fraude'],
                 marker='s', color='red', linestyle='--', label='Taxa real de fraude')

        plt.title(f'Partial Dependence vs Taxa Real - {feature}')
        plt.xlabel(feature)
        plt.ylabel('Probabilidade de fraude')
        plt.grid(True)
        plt.legend()
        plt.show()

    except Exception as e:
        print(f"Erro ao processar variável {feature}: {e}")

"""**Dados crus (taxa de fraude observada)**

* Suponha que, nos dados reais:

  * Usuários com tempo_sessao baixo (~100s) → 5% de fraude

  * Usuários com tempo_sessao médio (~400s) → 8% de fraude

  * Usuários com tempo_sessao muito alto (~1000s) → 30% de fraude

* Então, poderíamos concluir: “Quanto maior o tempo logado, maior a chance de fraude.”

**Detalhamento**

* O PDP pega cada observação da base e simula o que o modelo preveria se o tempo_sessao mudasse, mantendo as outras variáveis iguais.

* Vamos considerar o PDP para "tempo_sessao":
  * Ele pega uma lista de valores para tempo_sessao (por exemplo: 100, 400, 800, 1200 segundos).
  * Para cada valor, ele mantém todas as outras variáveis (tentativas_senha, dispositivos) iguais ao que estão na base e só troca tempo_sessao.
  * Ou seja:
    * Registro 1: tempo_sessao=800, tentativas_senha=0, dispositivos=1
    * Registro 2: tempo_sessao=800, tentativas_senha=1, dispositivos=1
    * Registro 3: tempo_sessao=800, tentativas_senha=2, dispositivos=1
    * Registro 4: tempo_sessao=800, tentativas_senha=0, dispositivos=2
    * Registro 5: tempo_sessao=800, tentativas_senha=1, dispositivos=2
    * Registro 6: tempo_sessao=800, tentativas_senha=2, dispositivos=2
    * Se nao houver uma combinação tentativas_senha=2, dispositivos=2, ele nao faz
  * E calcula a média de cada previsão

* Ao calcular a média das previsões para cada valor de tempo_sessao, o PDP pode mostrar, por exemplo:


| `tempo_sessao` (segundos) | Taxa real de fraude (%) | PDP - Modelo (%) |
| ------------------------- | ----------------------- | ---------------- |
| 100                       | 5                       | 7                |
| 400                       | 8                       | 10               |
| 800                       | 20                      | 12               |
| 1200                      | 30                      | 13               |


* Os dados reais sugerem que a fraude sobe muito acima de 800s.

* Mas o modelo não aprendeu isso com tanta força – ele acha que depois de 400s a chance de fraude fica quase estável (~12-13%).
"""